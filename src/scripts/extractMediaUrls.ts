import { readFileSync, writeFileSync, existsSync, readdirSync, appendFileSync } from 'fs';
import { join } from 'path';
import axios from 'axios';
import extractionConfig from '../config/extraction.config';

interface RedditPost {
  url: string;
  title: string;
  subreddit: string;
  author: string;
}

interface ExtractionResult {
  success: boolean;
  post: RedditPost;
  mediaUrls: string[];
  error?: string;
}

class MediaUrlExtractor {
  private config = extractionConfig;
  private extractedUrls: Map<string, string[]> = new Map(); // title -> URLs
  private failedRequests: RedditPost[] = [];
  private videoUrls: Set<string> = new Set();
  private processedCount = 0;

  constructor() {
    this.loadVideoUrls();
  }

  /**
   * Load video URLs to exclude them from processing
   */
  private loadVideoUrls(): void {
    const videoFile = this.config.files.videoUrls.all;
    if (existsSync(videoFile)) {
      const content = readFileSync(videoFile, 'utf8');
      const lines = content.split('\n').filter(line => line.trim() && !line.startsWith('#'));
      lines.forEach(url => this.videoUrls.add(url.trim()));
      console.log(`üìπ Loaded ${this.videoUrls.size} video URLs to exclude`);
    }
  }

  /**
   * Clear existing files and start fresh
   */
  private clearExistingFiles(): void {
    const filesToClear = [
      this.config.files.mediaUrls.all,
      this.config.files.mediaUrls.failedExtraction
    ];

    filesToClear.forEach(filename => {
      if (existsSync(filename)) {
        try {
          writeFileSync(filename, '', 'utf8');
          console.log(`üóëÔ∏è  Cleared existing file: ${filename}`);
        } catch (error) {
          console.log(`‚ö†Ô∏è  Could not clear ${filename}: ${error}`);
        }
      }
    });
  }

  /**
   * Read CSV files from reddit-links directory
   */
  private readCsvFiles(): RedditPost[] {
    const redditLinksDir = this.config.files.redditLinksDir;
    const posts: RedditPost[] = [];

    if (!existsSync(redditLinksDir)) {
      console.log(`‚ùå Directory not found: ${redditLinksDir}`);
      return posts;
    }

    const files = readdirSync(redditLinksDir).filter(file => file.endsWith('.csv'));
    console.log(`üìÅ Found ${files.length} CSV files in ${redditLinksDir}/`);

    for (const file of files) {
      const filePath = join(redditLinksDir, file);
      console.log(`üìÑ Processing: ${file}`);
      
      try {
        const content = readFileSync(filePath, 'utf8');
        const lines = content.split('\n').filter(line => line.trim());
        
        // Skip header if exists
        const dataLines = lines[0].includes('id') || lines[0].includes('url') ? lines.slice(1) : lines;
        
        for (const line of dataLines) {
          const columns = line.split(',');
          if (columns.length >= 2) {
            // Handle different CSV formats
            let url: string;
            let title: string;
            let subreddit: string;
            let author: string;

            if (columns[0]?.includes('http')) {
              // Format: url,title,subreddit,author
              url = columns[0]?.trim();
              title = columns[1]?.trim() || 'Unknown';
              subreddit = columns[2]?.trim() || 'Unknown';
              author = columns[3]?.trim() || 'Unknown';
            } else {
              // Format: id,permalink (extract info from permalink)
              url = columns[1]?.trim();
              const permalink = columns[1]?.trim();
              
              if (permalink) {
                // Extract subreddit from permalink: /r/subreddit/comments/id/title/
                const subredditMatch = permalink.match(/\/r\/([^\/]+)\/comments\//);
                subreddit = subredditMatch ? subredditMatch[1] : 'Unknown';
                
                // Extract title from permalink
                const titleMatch = permalink.match(/\/comments\/[^\/]+\/([^\/]+)/);
                title = titleMatch ? titleMatch[1].replace(/_/g, ' ') : 'Unknown';
                
                author = 'Unknown'; // Not available in this format
              } else {
                continue;
              }
            }

            if (url && title) {
              posts.push({ url, title, subreddit, author });
            }
          }
        }
      } catch (error) {
        console.error(`‚ùå Error reading ${file}:`, error);
      }
    }

    console.log(`üìä Total posts found: ${posts.length}`);
    return posts;
  }

  /**
   * Check if URL is a video (to exclude)
   */
  private isVideoUrl(url: string): boolean {
    return this.videoUrls.has(url) || 
           url.includes('v.redd.it') || 
           url.includes('redgifs.com') ||
           url.includes('.mp4') ||
           url.includes('.webm') ||
           url.includes('.mov');
  }

  /**
   * Check if URL is a media URL (image, gif, or text)
   */
  private isMediaUrl(url: string): boolean {
    // Skip video URLs
    if (this.isVideoUrl(url)) {
      return false;
    }
    
    // Check for image URLs
    if (url.includes('.jpg') || url.includes('.jpeg') || url.includes('.png') || 
        url.includes('.webp') || url.includes('.gif') || 
        url.includes('i.redd.it') || url.includes('i.imgur.com') ||
        url.includes('gfycat.com')) {
      return true;
    }
    
    // Check for text content (Reddit posts, comments, etc.)
    if (url.includes('reddit.com') && !url.includes('v.redd.it')) {
      return true;
    }
    
    return false;
  }

  /**
   * Extract media URLs from a Reddit post
   */
  private async extractMediaUrlsFromPost(post: RedditPost): Promise<ExtractionResult> {
    try {
      // Validate URL
      if (!post.url || !post.url.startsWith('http')) {
        return {
          success: false,
          post,
          mediaUrls: [],
          error: 'Invalid URL'
        };
      }

      // Add delay for rate limiting
      await new Promise(resolve => setTimeout(resolve, this.config.timing.delayBetweenRequests));
      
      const response = await axios.get(post.url, {
        headers: { 'User-Agent': this.config.userAgent },
        timeout: 30000
      });

      const html = response.data;
      const mediaUrls: string[] = [];
      
      // Extract URLs from the HTML
      const urlMatches = html.match(/https?:\/\/[^\s"']+/g) || [];
      
      for (const url of urlMatches) {
        if (this.isMediaUrl(url)) {
          mediaUrls.push(url);
        }
      }
      
      // Remove duplicates
      const uniqueUrls = [...new Set(mediaUrls)];
      
      console.log(`‚úÖ Extracted ${uniqueUrls.length} media URLs from: ${post.title}`);
      
      return {
        success: true,
        post,
        mediaUrls: uniqueUrls
      };
      
    } catch (error) {
      console.error(`‚ùå Failed to extract from ${post.title}:`, error);
      this.failedRequests.push(post);
      
      return {
        success: false,
        post,
        mediaUrls: [],
        error: error instanceof Error ? error.message : 'Request failed'
      };
    }
  }

  /**
   * Extract media URLs from all posts
   */
  async extractAllMediaUrls(): Promise<string[]> {
    this.clearExistingFiles();
    
    const posts = this.readCsvFiles();
    const allUrls: string[] = [];
    
    console.log(`üöÄ Starting media URL extraction for ${posts.length} posts`);
    console.log(`‚öôÔ∏è  Using config: ${this.config.batch.size} batch size, ${this.config.timing.delayBetweenRequests}ms delay, ${this.config.timing.delayBetweenBatches}ms batch delay`);
    
    for (let i = 0; i < posts.length; i++) {
      const post = posts[i];
      console.log(`\nüìù Processing ${i + 1}/${posts.length}: ${post.title}`);
      
      const result = await this.extractMediaUrlsFromPost(post);
      
      if (result.success && result.mediaUrls.length > 0) {
        this.extractedUrls.set(post.title, result.mediaUrls);
        allUrls.push(...result.mediaUrls);
        
        // Save incrementally every N iterations
        if ((i + 1) % this.config.batch.saveInterval === 0) {
          this.saveUrlsIncrementally(post.title, result.mediaUrls);
          console.log(`üíæ Saved progress: ${i + 1}/${posts.length} posts processed`);
        }
      } else if (!result.success) {
        this.failedRequests.push(post);
        this.saveFailedRequestIncrementally(post);
      }
      
      // Rate limiting: pause after every batch
      if ((i + 1) % this.config.batch.size === 0) {
        console.log(`‚è∏Ô∏è  Pausing for ${this.config.timing.delayBetweenBatches / 1000} seconds after batch ${Math.floor((i + 1) / this.config.batch.size)}`);
        await new Promise(resolve => setTimeout(resolve, this.config.timing.delayBetweenBatches));
      } else {
        // Small delay between requests
        await new Promise(resolve => setTimeout(resolve, this.config.timing.delayBetweenRequests));
      }
    }
    
    // Final save of all results
    this.saveUrlsToFile(this.extractedUrls, this.config.files.mediaUrls.all);
    this.saveFailedRequests();
    
    console.log(`\nüìä Extraction completed:`);
    console.log(`‚úÖ Posts with media: ${this.extractedUrls.size}`);
    console.log(`üìπ Total media URLs: ${allUrls.length}`);
    console.log(`‚ùå Failed requests: ${this.failedRequests.length}`);
    
    return allUrls;
  }

  /**
   * Save URLs to file in key-value format
   */
  saveUrlsToFile(urlsMap: Map<string, string[]>, filename: string): void {
    let content = '';
    
    for (const [title, urls] of urlsMap) {
      if (urls.length > 0) {
        content += `# ${title}\n`;
        urls.forEach(url => {
          content += `${url}\n`;
        });
        content += '\n';
      }
    }
    
    writeFileSync(filename, content, 'utf8');
    console.log(`üíæ Saved ${urlsMap.size} posts with media URLs to ${filename}`);
  }

  /**
   * Save URLs incrementally to file
   */
  private saveUrlsIncrementally(title: string, urls: string[]): void {
    if (urls.length === 0) return;
    
    const filename = this.config.files.mediaUrls.all;
    const escapedTitle = title.replace(/\n/g, '\\n').replace(/\r/g, '\\r');
    
    let content = `# ${escapedTitle}\n`;
    urls.forEach(url => content += `${url}\n`);
    content += '\n'; // Empty line between entries
    
    appendFileSync(filename, content, 'utf8');
  }

  /**
   * Save failed request incrementally
   */
  private saveFailedRequestIncrementally(post: RedditPost): void {
    const filename = this.config.files.mediaUrls.failedExtraction;
    const escapedTitle = post.title.replace(/\n/g, '\\n').replace(/\r/g, '\\r');
    
    const content = `# ${escapedTitle}\n${post.url}\n\n`;
    appendFileSync(filename, content, 'utf8');
  }

  /**
   * Save failed requests
   */
  saveFailedRequests(filename: string = this.config.files.mediaUrls.failedExtraction): void {
    if (this.failedRequests.length === 0) {
      console.log('‚úÖ No failed requests to save');
      return;
    }
    
    // Ensure the failed_requests directory exists
    const failedRequestsDir = this.config.files.failedRequestsDir;
    if (!existsSync(failedRequestsDir)) {
      mkdirSync(failedRequestsDir, { recursive: true });
    }
    
    let content = '';
    for (const post of this.failedRequests) {
      content += `# ${post.title}\n${post.url}\n\n`;
    }
    
    writeFileSync(filename, content, 'utf8');
    console.log(`üìù Saved ${this.failedRequests.length} failed requests to ${filename}`);
  }

  /**
   * Retry failed requests with 429 handling
   */
  async retryFailedPostsWith429Handling(failedPosts: RedditPost[]): Promise<{
    successful: number;
    failed: number;
    mediaFound: number;
  }> {
    if (failedPosts.length === 0) {
      console.log('‚úÖ No failed posts to retry');
      return { successful: 0, failed: 0, mediaFound: 0 };
    }
    
    console.log(`üîÑ Retrying ${failedPosts.length} failed posts...`);
    
    let successful = 0;
    let failed = 0;
    let mediaFound = 0;
    
    for (let i = 0; i < failedPosts.length; i++) {
      const post = failedPosts[i];
      console.log(`\nüîÑ Retrying ${i + 1}/${failedPosts.length}: ${post.title}`);
      
      try {
        // Longer delay for retries
        await new Promise(resolve => setTimeout(resolve, this.config.timing.retryDelay));
        
        const result = await this.extractMediaUrlsFromPost(post);
        
        if (result.success) {
          successful++;
          if (result.mediaUrls.length > 0) {
            this.extractedUrls.set(post.title, result.mediaUrls);
            mediaFound += result.mediaUrls.length;
          }
        } else {
          failed++;
        }
        
        // Rate limiting: pause after every 25 retries
        if ((i + 1) % 25 === 0) {
          console.log(`‚è∏Ô∏è  Pausing for ${this.config.timing.retryDelay / 1000} seconds after retry batch ${Math.floor((i + 1) / 25)}`);
          await new Promise(resolve => setTimeout(resolve, this.config.timing.retryDelay));
        }
        
      } catch (error) {
        failed++;
        console.error(`‚ùå Retry failed for ${post.title}:`, error);
      }
    }
    
    // Update the main results file
    this.saveUrlsToFile(this.extractedUrls, this.config.files.mediaUrls.all);
    
    console.log(`\nüìä Retry completed:`);
    console.log(`‚úÖ Successful retries: ${successful}`);
    console.log(`‚ùå Failed retries: ${failed}`);
    console.log(`üìπ Additional media URLs found: ${mediaFound}`);
    
    return { successful, failed, mediaFound };
  }

  /**
   * Retry failed requests from file
   */
  async retryFailedRequests(filename: string = this.config.files.mediaUrls.failedExtraction): Promise<string[]> {
    if (!existsSync(filename)) {
      console.log(`‚ùå File not found: ${filename}`);
      return [];
    }
    
    const content = readFileSync(filename, 'utf8');
    const lines = content.split('\n').filter(line => line.trim());
    
    const failedPosts: RedditPost[] = [];
    let currentPost: Partial<RedditPost> = {};
    
    for (const line of lines) {
      if (line.startsWith('# ')) {
        // Save previous post if exists
        if (currentPost.title && currentPost.url) {
          failedPosts.push(currentPost as RedditPost);
        }
        
        // Start new post
        currentPost = {
          title: line.substring(2),
          url: '',
          subreddit: 'Unknown',
          author: 'Unknown'
        };
      } else if (line.startsWith('http') && currentPost.title) {
        currentPost.url = line;
      }
    }
    
    // Add last post
    if (currentPost.title && currentPost.url) {
      failedPosts.push(currentPost as RedditPost);
    }
    
    console.log(`üìù Loaded ${failedPosts.length} failed posts from ${filename}`);
    
    const result = await this.retryFailedPostsWith429Handling(failedPosts);
    
    // Return all URLs found
    const allUrls: string[] = [];
    for (const urls of this.extractedUrls.values()) {
      allUrls.push(...urls);
    }
    
    return allUrls;
  }
}

async function main() {
  const extractor = new MediaUrlExtractor();
  
  try {
    const urls = await extractor.extractAllMediaUrls();
    
    console.log('\nüéâ Media URL extraction completed!');
    console.log(`üìä Total media URLs found: ${urls.length}`);
    console.log(`üìÅ Results saved to: ${extractionConfig.files.mediaUrls.all}`);
    
  } catch (error) {
    console.error('‚ùå Error during extraction:', error);
  }
}

// Run the main function
main(); 